{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a1f553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/srr_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.utils import set_seed\n",
    "from src.dataloaders import get_loaders, HARDataset\n",
    "from src.network import HARNet\n",
    "from src.pl_module import HARModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c8aec4",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "1) Быстрый взгляд на файлы с данными показал, что они сильно несбалансированы. В среднем, в файле около 6% объектов позитивного класса. Худшее значение 0 (в записи полностью отсутствует этап \"прием пищи\"), лучшее 15%. Это стоит иметь ввиду в дальнейшем решении.\n",
    "\n",
    "\n",
    "2) Пропуски, битые значение и т.д., замечены не были, по крайней мере лоадер и модель не ругались :)\n",
    "\n",
    "\n",
    "3) Вопиющих выбросов тоже замечено не было. Есть один подозрительный файл ('0001__R__2DBF__2019-10-08__104851__Accel-GYRO.npz'), в котором статистики по показаниям гироскопа сильно отличаются от всей остальной выборки. Его было решено исключить. \n",
    "\n",
    "\n",
    "4) Решаем задачу как бинарную классификацию, предварительно нарезав исходные данные \"окнами\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaafce7",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Мне привычнее работать с сетками и торчевским даталоадером, поэтому многие решения продиктованы именно этим. \n",
    "\n",
    "Для начала разбиваем имеющиеся данные на выборки по файлам. Это не совсем правильно с точки зрения идеальной валидации, но просто и надежно (только пришлось проверить, чтобы в валидационныую и тестовую выбору не попали исключительно записи без позитивного класса). Так же не делал стратификацию по данным, которые можно измвлечь из названия файлов, думаю рандом сплита хватит. Для начала - сойдет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b179b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "DATA_DIR = './data_compressed/'\n",
    "bad_file_name = '0001__R__2DBF__2019-10-08__104851__Accel-GYRO.npz'\n",
    "record_files = [x for x in os.listdir(DATA_DIR) if x!=bad_file_name]\n",
    "\n",
    "rec_train, rec_hold = train_test_split(record_files, test_size=0.2, random_state=10)\n",
    "rec_val, rec_test = train_test_split(rec_hold, test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e007016",
   "metadata": {},
   "source": [
    "# Что еще не было сделано:\n",
    "\n",
    "1) Нормализация, удаление выбросов, сглаживание, применение фильтров по частоте, более детальный анализ сырых данных. Так же неплохо было бы убедиться, что указанная частота снятия сэмплов соответствует действительности. \n",
    "\n",
    "\n",
    "2) При нарезке данных \"окнами\" была выбрана самая простая стратегия - без пересечений. Если делать окна со сдвигом, качество финального предсказания будет лучше.\n",
    "\n",
    "\n",
    "2) Дополнительный feature engineering. Например можно было посчитать разные производные показатели внутри окна, распарсить метаданные (время приема пищи) и тд. \n",
    "\n",
    "\n",
    "3) Постпроцессинг. Есть понимание, что после того, как сделали предсказание по окнам, нужно выделять непрерывные участки \"приема пищи\", это сильно повысит качество.\n",
    "\n",
    "\n",
    "4) Перебор всех возможных гиперпараметров и архитектуры сетки.\n",
    "\n",
    "\n",
    "5) Вынести отдельно конфиг, трекать эксперименты в логгер, сохранение/загрузка лучших чекпойнтов модели, применение более продвинутых оптимайзеров/шедулеров и вообще всякая инженерная составляющая. Это показалось излишним, при условии, что особо модель не нужно было фиттить (прогоняю одну эпоху)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72e3a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256 \n",
    "window_size = 60 # I think for a start, 6 seconds is close to the optimal value\n",
    "\n",
    "# it takes time\n",
    "train_loader, val_loader, test_loader = get_loaders(DATA_DIR, rec_train, rec_val, rec_test, \n",
    "                                                    batch_size, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f865a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HARNet(window_size)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "pl_module = HARModule(model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8efb1",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8308fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type              | Params\n",
      "----------------------------------------------\n",
      "0 | model   | HARNet            | 399 K \n",
      "1 | loss_fn | BCEWithLogitsLoss | 0     \n",
      "----------------------------------------------\n",
      "399 K     Trainable params\n",
      "0         Non-trainable params\n",
      "399 K     Total params\n",
      "1.598     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/srr_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:00<00:00,  2.32it/s]Val epoch Loss: 0.0027803427\n",
      "                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/srr_env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  43%|████▊      | 90/208 [02:12<02:53,  1.47s/it, loss=0.379, v_num=14]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=1, # I am limited in computing resources :(\n",
    "                     accelerator='cpu')\n",
    "trainer.fit(model=pl_module, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f9e72",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(model=pl_module, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da398c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "\n",
    "sig = nn.Sigmoid()\n",
    "\n",
    "y_prob = torch.cat([sig(x[0]) for x in output])\n",
    "y_gt = torch.cat([x[1] for x in output])\n",
    "y_pred = torch.where(y_prob > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47fac64",
   "metadata": {},
   "source": [
    "# Window-wise metrcis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(y_gt)\n",
    "plt.plot(y_pred, alpha=0.5)\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_gt, y_pred))\n",
    "print('ROC AUC:', roc_auc_score(y_gt, y_prob))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_gt, y_pred))\n",
    "disp.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078d6f8",
   "metadata": {},
   "source": [
    "# Point-wise metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fbb05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HARDataset(data_dir=DATA_DIR, rec_names=rec_test, window_size=window_size)\n",
    "\n",
    "test_df = test_dataset.get_origin_df()\n",
    "point_y_gt = test_df['target']\n",
    "point_y_pred = y_pred.repeat_interleave(60)\n",
    "point_y_prob = y_prob.repeat_interleave(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f085d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(point_y_gt)\n",
    "plt.plot(point_y_pred, alpha=0.5)\n",
    "plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aaed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(point_y_gt, point_y_pred))\n",
    "print('ROC AUC:', roc_auc_score(point_y_gt, point_y_prob))\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(point_y_gt, point_y_pred))\n",
    "disp.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74da856",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "Как можно заметить, решение вроде рабочее, можно сказать, что proof-of-concept получен)\n",
    "\n",
    "Из слабых мест - недостаточное время на обучение (одна эпоха, сетка далека от сходимости), плюс все что было перечислено ранее в блоке \"Что еще не было сделано\".\n",
    "\n",
    "Так же можно отметить, что агрессивный баланс классов сыграл злую шутку, пристуствует много ошибок второго рода, это тоже неплохо было бы пофиксить, а не просто выкручивать порог на очень высокие значения. \n",
    "\n",
    "В целом - было интересно сделать задание :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92320f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
